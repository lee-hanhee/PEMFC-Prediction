{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ea370c",
   "metadata": {},
   "source": [
    "# PyTorch Pressure Drop Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002dfdf",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226c5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "class RegressionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network for regression with five fully connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the neural network components.\n",
    "        \"\"\"\n",
    "        super(RegressionNN, self).__init__()  # Initialize the superclass\n",
    "        # Define the layers of the neural network\n",
    "        self.fc1 = nn.Linear(6, 256)  # First fully connected layer with 6 inputs and 128 outputs\n",
    "        self.fc2 = nn.Linear(256, 128)  # Second fully connected layer with 128 inputs and 64 outputs\n",
    "        self.fc3 = nn.Linear(128, 64)    # Output layer with 64 inputs and 1 output (regression)\n",
    "        self.fc4 = nn.Linear(64, 16)    # Output layer with 64 inputs and 1 output (regression)\n",
    "        self.fc5 = nn.Linear(16, 1)    # Output layer with 64 inputs and 1 output (regression)\n",
    "        self.relu = nn.ReLU()  # Activation function used between layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the neural network.\n",
    "        \"\"\"\n",
    "        x = self.relu(self.fc1(x))  # Pass input through the first layer and apply ReLU activation\n",
    "        x = self.relu(self.fc2(x))  # Pass through the second layer and apply ReLU activation\n",
    "        x = self.relu(self.fc3(x))  # Pass through the third layer and apply ReLU activation\n",
    "        x = self.relu(self.fc4(x))  # Pass through the fourth layer and apply ReLU activation\n",
    "        x = self.fc5(x)  # Output layer produces final output, no activation (linear output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0994e",
   "metadata": {},
   "source": [
    "## Load and Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6526760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_data(filepath):\n",
    "    \"\"\"\n",
    "    Load data from a file for training a machine learning model and convert it into tensors without scaling.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the data file. This file should be a whitespace-delimited text file and\n",
    "                        can contain comments starting with '%'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - X_tensor (Tensor): Tensor containing the raw features.\n",
    "            - Y_tensor (Tensor): Tensor containing the raw target variable.\n",
    "            Returns (None, None) if an error occurs during file loading or data processing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the file, ignoring lines with '%', which are comments.\n",
    "        data = pd.read_csv(filepath, delim_whitespace=True, comment='%', header=None)\n",
    "        \n",
    "        # Set custom column names for ease of access\n",
    "        data.columns = ['HCC', 'WCC', 'LCC', 'Tamb', 'Uin', 'Q',\n",
    "                        'Temperature1', 'Temperature2', 'Temperature3', 'Temperature4',\n",
    "                        'Temperature5', 'Temperature6', 'Pressure1', 'Pressure2', 'Delp',\n",
    "                        'Velocity1', 'Velocity2', 'Tsta', 'StackTemp2']\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the data file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Select specific columns for features and target\n",
    "    feature_labels = ['HCC', 'WCC', 'LCC', 'Tamb', 'Uin', 'Q']\n",
    "    X = data[feature_labels]\n",
    "    Y = data['Delp']\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab6e77",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808b2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(X, Y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e6c10",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c5264bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def preprocess_and_prepare_loaders(X_train, X_test, Y_train, Y_test, batch_size=64):\n",
    "    scaler_X = StandardScaler()\n",
    "\n",
    "    # Scale training data\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    \n",
    "    # Scale testing data using the same scaler\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "    # Save the scaler for potential inverse transformations later\n",
    "    joblib.dump(scaler_X, 'scaler_X.pkl')\n",
    "\n",
    "    # Convert scaled data to tensors. Use .values to correctly extract numpy arrays from pandas structures.\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Ensure that Y_train and Y_test, if they are pandas Series, are converted to tensors properly.\n",
    "    Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "    Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)   # Use .values to extract the numpy array\n",
    "\n",
    "    # Create TensorDataset and DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Print samples of scaled data for verification\n",
    "    print(\"Scaled Training Features Sample:\")\n",
    "    print(X_train_tensor[:5])\n",
    "    print(\"Training Targets Sample:\")\n",
    "    print(Y_train_tensor[:5])\n",
    "    print(\"Scaled Testing Features Sample:\")\n",
    "    print(X_test_tensor[:5])\n",
    "    print(\"Testing Targets Sample:\")\n",
    "    print(Y_test_tensor[:5])\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7558712",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc4036e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
    "    \"\"\"\n",
    "    Train a neural network model without learning rate scheduler.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        test_loader (DataLoader): DataLoader for validation data.\n",
    "        criterion (Loss function): The loss function.\n",
    "        optimizer (Optimizer): The optimizer.\n",
    "        num_epochs (int): Maximum number of epochs to train.\n",
    "        patience (int): Patience for early stopping.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        # Training phase\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = sum(criterion(model(inputs), labels).item() for inputs, labels in test_loader) / len(test_loader)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                break\n",
    "\n",
    "        # Optional: Simplified logging\n",
    "        if epoch % 10 == 0:  # Log every 10 epochs\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    print('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bd6d3",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c6069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a trained neural network model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        test_loader (DataLoader): DataLoader for the test data.\n",
    "        criterion (torch.nn.MSELoss): Loss function to use for evaluating the model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    total_loss = 0\n",
    "    total_rmse = 0\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)  # Compute model output\n",
    "\n",
    "            # Calculate loss for the batch directly with tensors\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Convert tensors to numpy for calculating RMSE and plotting\n",
    "            outputs_np = outputs.detach().cpu().numpy()\n",
    "            labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "            # Calculate RMSE for each batch\n",
    "            rmse = mean_squared_error(labels_np, outputs_np, squared=False)\n",
    "            total_rmse += rmse * labels.size(0)\n",
    "\n",
    "            # Store predictions and actuals for plotting\n",
    "            predictions.extend(outputs_np.flatten())\n",
    "            actuals.extend(labels_np.flatten())\n",
    "\n",
    "    # Compute the average loss and RMSE\n",
    "    mean_loss = total_loss / len(test_loader.dataset)\n",
    "    mean_rmse = total_rmse / len(test_loader.dataset)\n",
    "    relative_error = (mean_rmse / np.mean(actuals)) * 100  # Calculate relative error as a percentage\n",
    "    r2 = r2_score(actuals, predictions)  # Calculate R^2 score\n",
    "\n",
    "    # Print calculated metrics\n",
    "    print(f'Test MSE: {mean_loss:.4f}')\n",
    "    print(f'Test RMSE: {mean_rmse:.4f}')\n",
    "    print(f'Relative Error: {relative_error:.2f}%')\n",
    "    print(f'R^2 Score: {r2:.4f}')  # Print R^2 score\n",
    "\n",
    "    # Plot true vs predicted values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(actuals, predictions, alpha=0.5)\n",
    "    plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'k--', label='Ideal Line')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc7725",
   "metadata": {},
   "source": [
    "## Predict New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64b73380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_data_point(data_point, model_path, scaler_path):\n",
    "    # Load the trained model\n",
    "    model = RegressionNN()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Load the scaler\n",
    "    scaler_X = joblib.load(scaler_path)\n",
    "\n",
    "    # Scale the single data point\n",
    "    data_point_scaled = scaler_X.transform([data_point])  # Note: [data_point] makes it 2D\n",
    "\n",
    "    # Convert to tensor\n",
    "    data_point_tensor = torch.tensor(data_point_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():  # No need to track gradients for predictions\n",
    "        output = model(data_point_tensor)\n",
    "\n",
    "    prediction = output.numpy()  # Convert to numpy array if necessary\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71baed0",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c6e43fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[548.202]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanheelee/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Main function to run the training and evaluation process.\n",
    "    filepath = '/Users/hanheelee/Desktop/Projects/00_ML_Fuel_Cell/Set 1.txt'  # Path to data file\n",
    "    X, Y = load_and_prepare_data(filepath)\n",
    "    if X is None or Y is None:\n",
    "        print(\"Failed to load data.\")\n",
    "        return\n",
    "    \n",
    "    # Print samples for verification\n",
    "    print(\"Sample X (features):\")\n",
    "    print(X[:10])\n",
    "    print(\"Sample Y (targets):\")\n",
    "    print(Y[:10])\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, Y_train, Y_test = split_data(X,Y)\n",
    "    \n",
    "    # Print samples for verification\n",
    "    print(\"Sample of training features:\")\n",
    "    print(X_train.head())\n",
    "    print(\"Sample of training targets:\")\n",
    "    print(Y_train.head())\n",
    "    print(\"Sample of testing features:\")\n",
    "    print(X_test.head())\n",
    "    print(\"Sample of testing targets:\")\n",
    "    print(Y_test.head())\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_loader, test_loader = preprocess_and_prepare_loaders(X_train, X_test, Y_train, Y_test)\n",
    "    \n",
    "    # Create model instance\n",
    "    model = RegressionNN()  # Instantiate the model\n",
    "    criterion = nn.L1Loss()  # Loss function for regression\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-4,\n",
    "        betas=(0.9, 0.999),  # default betas for Adam\n",
    "        eps=1e-08,  # small epsilon for numerical stability\n",
    "        weight_decay=0.01,  # L2 regularization\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, test_loader, criterion, optimizer)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, test_loader, criterion)  # Pass the scaler_target to use for inverse scaling\n",
    "\n",
    "    # Save the model's state dictionary and scalers\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    \n",
    "    print(\"Model and scalers are saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "    main()  # Execute the main function if the script is run directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65beb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
